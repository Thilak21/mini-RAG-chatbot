{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDg+XF7z3UyuRD20HsDOYJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thilak21/mini-RAG-chatbot/blob/main/notebook/mini_RAG_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload Documents"
      ],
      "metadata": {
        "id": "Hnwykf38aS8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "HDAKIOTIUHrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and Spilt Documents"
      ],
      "metadata": {
        "id": "haojf2vzbLIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "yXkc5XfZdunG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = []\n",
        "for file in uploaded.keys():\n",
        "  print(\"Loading:\", file)\n",
        "  if file.endswith(\".pdf\"):\n",
        "    loader = PyPDFLoader(file)\n",
        "    docs = loader.load()\n",
        "    documents.extend(docs)\n",
        "\n",
        "  elif file.endswith(\".txt\"):\n",
        "    loader = TextLoader(file)\n",
        "    docs = loader.load()\n",
        "    documents.extend(docs)\n",
        "print(\"Total documents loaded:\", len(uploaded))\n",
        "print('Total pages:', len(documents))\n",
        "\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "\n",
        "chunks = splitter.split_documents(documents)\n",
        "texts = [chunk.page_content for chunk in chunks]\n",
        "\n",
        "print(\"Total Chunks:\", len(texts))"
      ],
      "metadata": {
        "id": "cg4lRQ7_eJ-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Embeddings"
      ],
      "metadata": {
        "id": "w_7fkMXofI7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "embeddings = embedding_model.encode(texts, batch_size=32, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "9v85phePfMZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store in FAISS"
      ],
      "metadata": {
        "id": "KY8pNsLkfvP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "E6TWGmImfz0y"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "embeddings = np.array(embeddings).astype('float32')\n",
        "index.add(embeddings)\n",
        "print(\"Vectors Stored:\", index.ntotal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_G-fZvOzf7QB",
        "outputId": "1aa9f49f-c1bd-4ba7-c496-bd4bc476f9d6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectors Stored: 950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check all the vectors,chunks & embedding are retrieved same"
      ],
      "metadata": {
        "id": "sGVial2lmrjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Documents:\", len(documents))\n",
        "print(\"Vectors:\", index.ntotal)\n",
        "print(\"chunks:\", len(chunks))\n",
        "print(\"Embeddings:\", len(embeddings))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyBxe0NqmcJj",
        "outputId": "63c8f6e2-a07f-450d-a7fa-0a8fc80c4a14"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents: 87\n",
            "Vectors: 950\n",
            "chunks: 950\n",
            "Embeddings: 950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Hugging Face LLM"
      ],
      "metadata": {
        "id": "gttHoXUJiWQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = \"google/flan-t5-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "YQiszyYVibcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bulid Retrival and Generation function"
      ],
      "metadata": {
        "id": "VciQf2mDkodJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(query, k=4):\n",
        "\n",
        "  query_embedding = embedding_model.encode([query])\n",
        "  query_embedding = np.array(query_embedding).astype('float32')\n",
        "  distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "  retrieved_chunks = [texts[i] for i in indices[0]]\n",
        "  context = \"\\n\\n\".join(retrieved_chunks)\n",
        "  context = context[:1000]\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  Answer only using the provided context .if the answer is not in the context, say i don't know.\n",
        "    Context:\n",
        "    {context}\n",
        "    Question:\n",
        "    {query}\n",
        "    Answer :\n",
        "    \"\"\"\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors=\"pt\", truncation = True)\n",
        "\n",
        "  outputs = model.generate(\n",
        "          **inputs,\n",
        "          max_new_tokens=150,\n",
        "          temperature = 0.3)\n",
        "\n",
        "  answer = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
        "  return answer"
      ],
      "metadata": {
        "id": "caahArW4QzBj"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create gradio ui"
      ],
      "metadata": {
        "id": "hIeg-FDUnhAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chatbot(query):\n",
        "  return answer_question(query)\n",
        "\n",
        "interface = gr.Interface(\n",
        "       fn=chatbot,\n",
        "       inputs=\"text\",\n",
        "       outputs=\"text\",\n",
        "       title=\"Mini RAG Chatbot\",\n",
        "       description=\"Ask questions based on uploaded documents\"\n",
        ")\n",
        "\n",
        "interface.launch(\n",
        "    share=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "ZqiW_bfcnllJ",
        "outputId": "d3937d08-e2c9-424e-fa7f-bb3d46309a21"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5ed83c695939b071b7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5ed83c695939b071b7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}